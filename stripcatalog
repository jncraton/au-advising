#! /usr/bin/python3

import sys
import re
from tokenizers import Tokenizer

def load_tokenizer():
    """Load the Gemma 3 tokenizer from Hugging Face."""
    try:
        tokenizer = Tokenizer.from_pretrained("jncraton/gemma-3-270m-ct2-int8")
        return tokenizer
    except Exception as e:
        print(f"Warning: Could not load tokenizer: {e}")
        return None


def count_tokens(text, tokenizer):
    """Count tokens in the given text using the provided tokenizer."""
    if tokenizer is None:
        return 0
    try:
        encoding = tokenizer.encode(text)
        return len(encoding.ids)
    except Exception as e:
        print(f"Warning: Could not count tokens: {e}")
        return 0

drop_sections = [
  'table of contents',
  'Letter from the President',
  'HISTORICAL PERSPECTIVE',
  'MISSION',
  'LIVING THE MISSION',
  'ETHOS STATEMENT',
  'index',
  'Preamble',
  'At Our Core',
  'CORE VALUES',
  'COMMITMENT TO ACADEMIC INQUIRY',
  'ACCREDITATIONS AND RELATIONSHIPS',
]

tokenizer = load_tokenizer()

for file in sys.argv[1:]:
  text = open(file).read()

  initial_size = len(text)
  initial_tokens = count_tokens(text, tokenizer)

  for heading in drop_sections:
    text = re.sub(r"(#{1,6})\s*(" + heading + r")\s*\n(.*?)(?=(?:\n^#{1,6}\s)|\Z)", "", text, flags=re.I|re.M|re.DOTALL)

  text = re.sub(r"## UNIVERSITY TRUSTEES.*", "", text, flags=re.I|re.DOTALL)

  text = re.sub("\n\n\n+", "\n\n", text)

  tokens = count_tokens(text, tokenizer)
  print(f"{file} bytes: {initial_size} => {len(text)} tokens: {initial_tokens} => {tokens}", file=sys.stderr)

  print(text)
