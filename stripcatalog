#! /usr/bin/python3

import sys
import re
from tokenizers import Tokenizer


def load_tokenizer():
    """Load the Gemma 3 tokenizer from Hugging Face."""
    try:
        tokenizer = Tokenizer.from_pretrained("jncraton/gemma-3-270m-ct2-int8")
        return tokenizer
    except Exception as e:
        print(f"Warning: Could not load tokenizer: {e}")
        return None


def count_tokens(text, tokenizer):
    """Count tokens in the given text using the provided tokenizer."""
    if tokenizer is None:
        return 0
    try:
        encoding = tokenizer.encode(text)
        return len(encoding.ids)
    except Exception as e:
        print(f"Warning: Could not count tokens: {e}")
        return 0


drop_sections = [
    "table of contents",
    "Letter from the President",
    "HISTORICAL PERSPECTIVE",
    "MISSION",
    "LIVING THE MISSION",
    "ETHOS STATEMENT",
    "index",
    "Preamble",
    "At Our Core",
    "CORE VALUES",
    "COMMITMENT TO ACADEMIC INQUIRY",
    "ACCREDITATIONS AND RELATIONSHIPS",
]

to_title_case = [
    "PREREQUISITE",
    "COREQUISITE",
    "OFFERED",
    "EXPECTATION",
    "CONSENT",
    "REPEAT",
    "GRADE",
]

tokenizer = load_tokenizer()

for file in sys.argv[1:]:
    with open(file, "r", encoding="utf-8") as fh:
        text = fh.read()

    initial_size = len(text)
    initial_tokens = count_tokens(text, tokenizer)

    for heading in drop_sections:
        text = re.sub(
            r"(#{1,6})\s*(" + heading + r")\s*\n(.*?)(?=(?:\n^#{1,6}\s)|\Z)",
            "",
            text,
            flags=re.I | re.M | re.DOTALL,
        )

    text = re.sub(r"#+ UNIVERSITY TRUSTEES.*", "", text, flags=re.I | re.DOTALL)
    text = re.sub(r"#+ OFFICERS OF THE CORPORATION.*", "", text, flags=re.I | re.DOTALL)

    text = re.sub(r"#+.*cont\..*", "", text, flags=re.I)

    text = re.sub(r"#+\s*([\d\-]+\s*hour.*)", r"\1", text, flags=re.I)

    text = re.sub(r"(#+.*):", r"\1", text, flags=re.I)

    text = re.sub("\n\n\n+", "\n\n", text)

    for word in to_title_case:
        text = re.sub(rf"\b{re.escape(word)}\b", word.title(), text)

    text = re.sub(rf"\bSem\.", "Semester", text)
    text = re.sub(rf"\bhrs\.", "hours", text)
    text = re.sub(rf"\bhr\.", "hour", text)

    text = re.sub(r"\.\n", "\n", text)

    text = re.sub(r"\n- o ", "\n  - ", text)

    text = re.sub(r"(\d),\s+(\d)", r"\1,\2", text)
    
    text = re.sub(r"\s+\n", r"\n", text)

    tokens = count_tokens(text, tokenizer)
    print(
        f"{file} bytes: {initial_size} => {len(text)} tokens: {initial_tokens} => {tokens}",
        file=sys.stderr,
    )

    with open(file, "w", encoding="utf-8") as fh:
        fh.write(text)
