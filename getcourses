#!/usr/bin/python3

import os
import pandas as pd


def scrape_largest_table(url, drop_cols):
    # fetch all tables from the url and return the largest table
    try:
        tables = pd.read_html(url)
        if not tables:
            return None

        largest = max(tables, key=lambda df: (df.shape[0], df.shape[1]))
        df = largest.fillna("")
        df.columns = df.columns.astype(str)
        to_drop = [c for c in df.columns if c.lower() in {d.lower() for d in drop_cols}]
        df_filtered = df.drop(columns=to_drop, errors="ignore")
        return df_filtered
    except Exception:
        return None


if __name__ == "__main__":
    # Note that prereqs do not work here currently
    drop_cols = ['term', 'class section', 'start dt', 'end dt', 'class nbr', 'sectn req', 'topic', 'instr', 'grd', 'department', 'prereqs']
    term_names = ["", "fall", "", "spring", "summer"]
    for term in ["2251", "2253", "2261", "2263"]:
        target_url = f"https://basilisk.anderson.edu:4500/under_graduates.aspx?ListBox3={term}"

        year = int(term[1:3])
        filename = f"references/20{year}-{year+1}/schedule-{term_names[int(term[-1])]}.tsv"
        os.makedirs(os.path.dirname(filename), exist_ok=True)

        df = scrape_largest_table(target_url, drop_cols)
        if df is None:
            # create an empty file to signal failure
            open(filename, "w").close()
        else:
            df.to_csv(filename, sep="\t", index=False, encoding="utf-8")
